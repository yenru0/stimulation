{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "from function.layer import *\n",
    "from function.etc import Trainer\n",
    "from function.optimizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"\n",
    "    다음과 같은 CNN을 구성한다.\n",
    "    → Conv → ReLU → Pooling → Affine → ReLU → Affine → Softmax →\n",
    "    전체 구현은 simple_convnet.py 참고\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        # 초기화 인수로 주어진 하이퍼파라미터를 딕셔너리에서 꺼내고 출력 크기를 계산한다.\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "            filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "\n",
    "        # 1층의 합성곱 계층과 2, 3층의 완전연결 계층의 가중치와 편향 생성\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # CNN을 구성하는 계층을 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"추론을 수행\"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실함수 값 계산\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"오차역전파법으로 기울기를 구함\"\"\"\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299029886231992\n",
      "=== epoch:1, train acc:0.405, test acc:0.486 ===\n",
      "train loss:2.296333266533969\n",
      "train loss:2.2910494670158355\n",
      "train loss:2.2802446118282056\n",
      "train loss:2.272395891867908\n",
      "train loss:2.2597606773165224\n",
      "train loss:2.242454839754824\n",
      "train loss:2.215094773364906\n",
      "train loss:2.1841805522276987\n",
      "train loss:2.182291313937086\n",
      "train loss:2.148951354035018\n",
      "train loss:2.086237297831817\n",
      "train loss:2.055436687169942\n",
      "train loss:1.9803704382750789\n",
      "train loss:1.9457713042326374\n",
      "train loss:1.8468408718067106\n",
      "train loss:1.7547334818113243\n",
      "train loss:1.6204709571703269\n",
      "train loss:1.69114255190232\n",
      "train loss:1.4655353273258618\n",
      "train loss:1.4622850666962628\n",
      "train loss:1.3515836556089302\n",
      "train loss:1.1350299905026615\n",
      "train loss:1.148793450287502\n",
      "train loss:1.1231845235712292\n",
      "train loss:1.0873308482369894\n",
      "train loss:1.0325382591223893\n",
      "train loss:0.9589116045472561\n",
      "train loss:1.0252425270932917\n",
      "train loss:0.8027812931444251\n",
      "train loss:0.9058703970494197\n",
      "train loss:0.7981561195192941\n",
      "train loss:0.9350637709708064\n",
      "train loss:0.6687302994001455\n",
      "train loss:0.7024033497347435\n",
      "train loss:0.7497313272604897\n",
      "train loss:0.6468848591523145\n",
      "train loss:0.8030397979389354\n",
      "train loss:0.6318625113100405\n",
      "train loss:0.6188007838224049\n",
      "train loss:0.6065868782992533\n",
      "train loss:0.6903777660870323\n",
      "train loss:0.5726677021331428\n",
      "train loss:0.5166623566905462\n",
      "train loss:0.5899612984603195\n",
      "train loss:0.5193871750349078\n",
      "train loss:0.7195523074060111\n",
      "train loss:0.5509278131329713\n",
      "train loss:0.7475628429387636\n",
      "train loss:0.43689665265052186\n",
      "train loss:0.5817567777535605\n",
      "=== epoch:2, train acc:0.821, test acc:0.816 ===\n",
      "train loss:0.5280427882627649\n",
      "train loss:0.6309628987857865\n",
      "train loss:0.6405386193520515\n",
      "train loss:0.4750499683658447\n",
      "train loss:0.3938052257178661\n",
      "train loss:0.5792954824166705\n",
      "train loss:0.48246474813498247\n",
      "train loss:0.2965155275337831\n",
      "train loss:0.5067076207789472\n",
      "train loss:0.5485029348688856\n",
      "train loss:0.45884733392757626\n",
      "train loss:0.3652011202332312\n",
      "train loss:0.5099578769521758\n",
      "train loss:0.43977319829219985\n",
      "train loss:0.6011742759865356\n",
      "train loss:0.4069331441647671\n",
      "train loss:0.33781542996405356\n",
      "train loss:0.4345766368099597\n",
      "train loss:0.3962199785205771\n",
      "train loss:0.5563059014274454\n",
      "train loss:0.3465032404907095\n",
      "train loss:0.5693613927089671\n",
      "train loss:0.3549221779501742\n",
      "train loss:0.33266616747952893\n",
      "train loss:0.32829757700622125\n",
      "train loss:0.3606526124376175\n",
      "train loss:0.3297390184047677\n",
      "train loss:0.25193917081478323\n",
      "train loss:0.35336273684188213\n",
      "train loss:0.5225465267403544\n",
      "train loss:0.2094202471963016\n",
      "train loss:0.3743064158485811\n",
      "train loss:0.4929167988757517\n",
      "train loss:0.4645093193985364\n",
      "train loss:0.32090664030346056\n",
      "train loss:0.28428863563078\n",
      "train loss:0.3455905434215131\n",
      "train loss:0.3132276327323489\n",
      "train loss:0.35767134195400885\n",
      "train loss:0.4067846375803782\n",
      "train loss:0.3137168957766792\n",
      "train loss:0.4718730054547086\n",
      "train loss:0.23145157027666138\n",
      "train loss:0.29282635233503573\n",
      "train loss:0.31740479037743957\n",
      "train loss:0.612900542841367\n",
      "train loss:0.3746378842464411\n",
      "train loss:0.5206571570483408\n",
      "train loss:0.27993472437252875\n",
      "train loss:0.4503089622588103\n",
      "=== epoch:3, train acc:0.865, test acc:0.861 ===\n",
      "train loss:0.211789830984425\n",
      "train loss:0.397879885587014\n",
      "train loss:0.4420469748169781\n",
      "train loss:0.3262950664305708\n",
      "train loss:0.352358247147569\n",
      "train loss:0.22341903874885571\n",
      "train loss:0.36047032095224607\n",
      "train loss:0.5333325002852747\n",
      "train loss:0.3285271427562592\n",
      "train loss:0.31024983913228693\n",
      "train loss:0.27092231519520205\n",
      "train loss:0.3090973366160367\n",
      "train loss:0.37903371205759234\n",
      "train loss:0.32186862662454063\n",
      "train loss:0.29306885831659907\n",
      "train loss:0.44192524867321614\n",
      "train loss:0.26725608871163775\n",
      "train loss:0.3590519752013428\n",
      "train loss:0.31716864175549253\n",
      "train loss:0.3429187348501677\n",
      "train loss:0.18964195856384708\n",
      "train loss:0.3710417435563676\n",
      "train loss:0.4236822291078644\n",
      "train loss:0.20189955675070126\n",
      "train loss:0.39431036529194136\n",
      "train loss:0.1829741954993219\n",
      "train loss:0.2773544290853507\n",
      "train loss:0.33854555960119015\n",
      "train loss:0.24054741205520472\n",
      "train loss:0.25027080797606\n",
      "train loss:0.3555229048165196\n",
      "train loss:0.4686092312701614\n",
      "train loss:0.431763389570072\n",
      "train loss:0.16799474541671222\n",
      "train loss:0.25402154841054464\n",
      "train loss:0.2553307358857026\n",
      "train loss:0.4169479292596873\n",
      "train loss:0.3029588138283895\n",
      "train loss:0.34960111318308146\n",
      "train loss:0.30426659980643306\n",
      "train loss:0.2920510075173024\n",
      "train loss:0.2655498790573469\n",
      "train loss:0.21631329302879224\n",
      "train loss:0.2630465806531962\n",
      "train loss:0.36562884958342096\n",
      "train loss:0.2673138389726582\n",
      "train loss:0.2282679306205145\n",
      "train loss:0.23222631575650474\n",
      "train loss:0.30171100377425264\n",
      "train loss:0.14828840690801462\n",
      "=== epoch:4, train acc:0.897, test acc:0.885 ===\n",
      "train loss:0.3309006000565961\n",
      "train loss:0.5277611594594019\n",
      "train loss:0.34346066459033664\n",
      "train loss:0.24419490520230394\n",
      "train loss:0.3055657149641901\n",
      "train loss:0.17462844563920965\n",
      "train loss:0.37202813467149914\n",
      "train loss:0.33753957218927233\n",
      "train loss:0.3259241513285294\n",
      "train loss:0.4033587363012588\n",
      "train loss:0.19647170428200972\n",
      "train loss:0.2733273182510526\n",
      "train loss:0.27674108259999186\n",
      "train loss:0.5287977915473923\n",
      "train loss:0.2782821205388582\n",
      "train loss:0.21055886550153793\n",
      "train loss:0.34891129616165456\n",
      "train loss:0.204269333617945\n",
      "train loss:0.32221819947337343\n",
      "train loss:0.45031078211282494\n",
      "train loss:0.31692007553495904\n",
      "train loss:0.22414643157459238\n",
      "train loss:0.24459669441239984\n",
      "train loss:0.1626946539958349\n",
      "train loss:0.20356023777750284\n",
      "train loss:0.1962323952841496\n",
      "train loss:0.14954902208205456\n",
      "train loss:0.23111568052545728\n",
      "train loss:0.14604758293645997\n",
      "train loss:0.198085155850184\n",
      "train loss:0.21022372282525237\n",
      "train loss:0.11486763515044482\n",
      "train loss:0.19711387775721143\n",
      "train loss:0.4996340969479749\n",
      "train loss:0.24766112749801553\n",
      "train loss:0.29286834390624583\n",
      "train loss:0.20406882299752888\n",
      "train loss:0.3403699581958982\n",
      "train loss:0.14646490385466632\n",
      "train loss:0.15931783748574377\n",
      "train loss:0.4828471562976197\n",
      "train loss:0.24565168847711422\n",
      "train loss:0.2910962547127043\n",
      "train loss:0.15405981832682147\n",
      "train loss:0.3219361673126428\n",
      "train loss:0.3966320056655902\n",
      "train loss:0.33748236500364065\n",
      "train loss:0.25780757204362426\n",
      "train loss:0.2099998614763982\n",
      "train loss:0.18717488862838988\n",
      "=== epoch:5, train acc:0.905, test acc:0.896 ===\n",
      "train loss:0.3091834295958637\n",
      "train loss:0.26033699334574933\n",
      "train loss:0.25403809529516147\n",
      "train loss:0.21848645866274805\n",
      "train loss:0.302582407576786\n",
      "train loss:0.2703752431124426\n",
      "train loss:0.24082252059925832\n",
      "train loss:0.22670420606998565\n",
      "train loss:0.25355090883709425\n",
      "train loss:0.24666589270162473\n",
      "train loss:0.16961668581751105\n",
      "train loss:0.30459077854602973\n",
      "train loss:0.15457254674144344\n",
      "train loss:0.22572363086158861\n",
      "train loss:0.4726843961630488\n",
      "train loss:0.21170486017651097\n",
      "train loss:0.2356362335924601\n",
      "train loss:0.3370203269401927\n",
      "train loss:0.2387236969649031\n",
      "train loss:0.10517809131675009\n",
      "train loss:0.13209257914712183\n",
      "train loss:0.13117115775000948\n",
      "train loss:0.24695831533910315\n",
      "train loss:0.25182995483593307\n",
      "train loss:0.2027617399735198\n",
      "train loss:0.3264090278623397\n",
      "train loss:0.18655247286021243\n",
      "train loss:0.3171030371598289\n",
      "train loss:0.12310036042464013\n",
      "train loss:0.20490500350986732\n",
      "train loss:0.29288956850384595\n",
      "train loss:0.16920414649406917\n",
      "train loss:0.12437982084515106\n",
      "train loss:0.25384599693068016\n",
      "train loss:0.24712450729200616\n",
      "train loss:0.21963587637667603\n",
      "train loss:0.1049798880462912\n",
      "train loss:0.24048756593766096\n",
      "train loss:0.24527203798642605\n",
      "train loss:0.08825852742197504\n",
      "train loss:0.15826576864678332\n",
      "train loss:0.251175088223728\n",
      "train loss:0.16883034188883994\n",
      "train loss:0.15153838294764488\n",
      "train loss:0.36152957701291555\n",
      "train loss:0.19274451860937486\n",
      "train loss:0.19883315613228583\n",
      "train loss:0.11810518791552135\n",
      "train loss:0.17799309860323745\n",
      "train loss:0.16421952306040832\n",
      "=== epoch:6, train acc:0.927, test acc:0.926 ===\n",
      "train loss:0.22495596777096474\n",
      "train loss:0.28675082693813064\n",
      "train loss:0.1933696193619406\n",
      "train loss:0.14019783992487503\n",
      "train loss:0.25348095931993514\n",
      "train loss:0.08404110232285161\n",
      "train loss:0.1777850732364244\n",
      "train loss:0.14165874414773744\n",
      "train loss:0.1261888996860947\n",
      "train loss:0.166202563899283\n",
      "train loss:0.19292922903557744\n",
      "train loss:0.14450570615076966\n",
      "train loss:0.28869767926417855\n",
      "train loss:0.12468527800027492\n",
      "train loss:0.12949474090570687\n",
      "train loss:0.12380105174157524\n",
      "train loss:0.19685044253473064\n",
      "train loss:0.24701876926911187\n",
      "train loss:0.14713346187696336\n",
      "train loss:0.13313070258686166\n",
      "train loss:0.1916285589300759\n",
      "train loss:0.15104084006513138\n",
      "train loss:0.17909169792005944\n",
      "train loss:0.25041776115993797\n",
      "train loss:0.17854130407188065\n",
      "train loss:0.07405536846316753\n",
      "train loss:0.14923348281744198\n",
      "train loss:0.10934271337531504\n",
      "train loss:0.11804109560019097\n",
      "train loss:0.10106077779334115\n",
      "train loss:0.18584754191825412\n",
      "train loss:0.13965035752825936\n",
      "train loss:0.18381817104894582\n",
      "train loss:0.15232973654026508\n",
      "train loss:0.2344506297581809\n",
      "train loss:0.24490802101630613\n",
      "train loss:0.0959053575912052\n",
      "train loss:0.13950514507720993\n",
      "train loss:0.2767732028253236\n",
      "train loss:0.10717009235372671\n",
      "train loss:0.29550848173726957\n",
      "train loss:0.18158303956977753\n",
      "train loss:0.1695507143462217\n",
      "train loss:0.29550853359174517\n",
      "train loss:0.14065938666688044\n",
      "train loss:0.16306839706928966\n",
      "train loss:0.2503497752636342\n",
      "train loss:0.25194262681590446\n",
      "train loss:0.1074520478482654\n",
      "train loss:0.30405574627199194\n",
      "=== epoch:7, train acc:0.935, test acc:0.924 ===\n",
      "train loss:0.23795990163981262\n",
      "train loss:0.26771364683845533\n",
      "train loss:0.1448251743957684\n",
      "train loss:0.18266461615593702\n",
      "train loss:0.09130641389219163\n",
      "train loss:0.23713166845021177\n",
      "train loss:0.178608174689757\n",
      "train loss:0.13631877490633595\n",
      "train loss:0.15084207323073925\n",
      "train loss:0.16469654346960547\n",
      "train loss:0.18036519950605914\n",
      "train loss:0.1902998802613342\n",
      "train loss:0.14998267481041863\n",
      "train loss:0.16713094732318343\n",
      "train loss:0.1018626872323285\n",
      "train loss:0.12674497517494585\n",
      "train loss:0.20484285383777476\n",
      "train loss:0.1654254589724015\n",
      "train loss:0.1248501712731326\n",
      "train loss:0.09754242557425422\n",
      "train loss:0.13946559083124957\n",
      "train loss:0.0930941616014931\n",
      "train loss:0.17814008237968754\n",
      "train loss:0.2065932573850855\n",
      "train loss:0.15609796449715094\n",
      "train loss:0.22273074619647087\n",
      "train loss:0.18215930493915344\n",
      "train loss:0.08281707416379593\n",
      "train loss:0.14513495023886874\n",
      "train loss:0.21997236725865904\n",
      "train loss:0.13738968329292883\n",
      "train loss:0.20854313523629453\n",
      "train loss:0.13153556012389223\n",
      "train loss:0.06599222578533968\n",
      "train loss:0.19879855458589737\n",
      "train loss:0.1315432577723063\n",
      "train loss:0.12433140140689841\n",
      "train loss:0.0493239278629226\n",
      "train loss:0.1190740542584846\n",
      "train loss:0.10397832482384273\n",
      "train loss:0.16734687966275835\n",
      "train loss:0.09556694654777764\n",
      "train loss:0.23969295003328303\n",
      "train loss:0.16014435895519097\n",
      "train loss:0.08930099802671285\n",
      "train loss:0.14293736657095615\n",
      "train loss:0.16817565096864823\n",
      "train loss:0.10834329689664475\n",
      "train loss:0.2623657210536379\n",
      "train loss:0.19871160441680147\n",
      "=== epoch:8, train acc:0.947, test acc:0.928 ===\n",
      "train loss:0.16245439325989153\n",
      "train loss:0.15910813634220417\n",
      "train loss:0.11526086001303329\n",
      "train loss:0.18068848504332446\n",
      "train loss:0.11526888668895748\n",
      "train loss:0.16927779650309926\n",
      "train loss:0.13741499683570477\n",
      "train loss:0.1357755821122123\n",
      "train loss:0.07532696713348777\n",
      "train loss:0.12313753372151842\n",
      "train loss:0.07583165571122505\n",
      "train loss:0.11786757838537264\n",
      "train loss:0.1465275021595105\n",
      "train loss:0.24698861541787356\n",
      "train loss:0.1335032803738377\n",
      "train loss:0.16858474393419587\n",
      "train loss:0.06442065979091813\n",
      "train loss:0.14733800249239104\n",
      "train loss:0.07929591158137739\n",
      "train loss:0.1083132995517475\n",
      "train loss:0.0612636660838411\n",
      "train loss:0.18017122576834216\n",
      "train loss:0.23194764291998124\n",
      "train loss:0.08637145886060962\n",
      "train loss:0.21851195321708702\n",
      "train loss:0.09314693222602104\n",
      "train loss:0.22622316517821647\n",
      "train loss:0.2129685677378269\n",
      "train loss:0.0940371993745752\n",
      "train loss:0.0926652306603964\n",
      "train loss:0.1355101694627839\n",
      "train loss:0.10659486937196862\n",
      "train loss:0.23349873683596517\n",
      "train loss:0.08207919287672981\n",
      "train loss:0.12455176034756973\n",
      "train loss:0.13182902449851652\n",
      "train loss:0.0787258057575041\n",
      "train loss:0.09352213571994986\n",
      "train loss:0.09409155073095804\n",
      "train loss:0.11432006561396008\n",
      "train loss:0.1649409338234449\n",
      "train loss:0.09675421337525199\n",
      "train loss:0.14305029503383115\n",
      "train loss:0.05969462698873025\n",
      "train loss:0.0744110999498628\n",
      "train loss:0.14887186746640435\n",
      "train loss:0.1264355621433163\n",
      "train loss:0.10901576317953877\n",
      "train loss:0.16058504224742964\n",
      "train loss:0.15589775369795278\n",
      "=== epoch:9, train acc:0.949, test acc:0.927 ===\n",
      "train loss:0.14522375451906455\n",
      "train loss:0.07789678294193048\n",
      "train loss:0.1962964712917045\n",
      "train loss:0.1501211078980478\n",
      "train loss:0.1459181714592652\n",
      "train loss:0.13165993750396274\n",
      "train loss:0.16750169424040023\n",
      "train loss:0.11435282492905573\n",
      "train loss:0.182741736673262\n",
      "train loss:0.09099276083163184\n",
      "train loss:0.1152022447958082\n",
      "train loss:0.07243946525803566\n",
      "train loss:0.1499463960584987\n",
      "train loss:0.15870162858334755\n",
      "train loss:0.14190037213452208\n",
      "train loss:0.11460519792380229\n",
      "train loss:0.10627258432355897\n",
      "train loss:0.11154848512667931\n",
      "train loss:0.19031722553349428\n",
      "train loss:0.09907714473542635\n",
      "train loss:0.13839646537518002\n",
      "train loss:0.09244740133906935\n",
      "train loss:0.08850178192910578\n",
      "train loss:0.05673256433270253\n",
      "train loss:0.09413395481578384\n",
      "train loss:0.15986515577614394\n",
      "train loss:0.08761569751722785\n",
      "train loss:0.13627628096646036\n",
      "train loss:0.09810695624914058\n",
      "train loss:0.08978065536992146\n",
      "train loss:0.15229678450459416\n",
      "train loss:0.1132792505898546\n",
      "train loss:0.05809115148482011\n",
      "train loss:0.11821954680838725\n",
      "train loss:0.04685817596450823\n",
      "train loss:0.08788873266503129\n",
      "train loss:0.044014820965940116\n",
      "train loss:0.06185604363898772\n",
      "train loss:0.10406996806298008\n",
      "train loss:0.06766949341112537\n",
      "train loss:0.16035104692840205\n",
      "train loss:0.16867167427475402\n",
      "train loss:0.04197725973240309\n",
      "train loss:0.051676218532593354\n",
      "train loss:0.08040157058533644\n",
      "train loss:0.11096692658227329\n",
      "train loss:0.17852582264500627\n",
      "train loss:0.11513282245979434\n",
      "train loss:0.08677307892481925\n",
      "train loss:0.09949867623359014\n",
      "=== epoch:10, train acc:0.961, test acc:0.941 ===\n",
      "train loss:0.09363483616838501\n",
      "train loss:0.18107086330318364\n",
      "train loss:0.20501833473066228\n",
      "train loss:0.10939293850098554\n",
      "train loss:0.04725623464057648\n",
      "train loss:0.16542451255682322\n",
      "train loss:0.0741986311440454\n",
      "train loss:0.15795177527897217\n",
      "train loss:0.07478702682291717\n",
      "train loss:0.07030909850215637\n",
      "train loss:0.06302287315850491\n",
      "train loss:0.10198948249320482\n",
      "train loss:0.14118478973024\n",
      "train loss:0.12042464638535584\n",
      "train loss:0.0831026660251901\n",
      "train loss:0.06720268633140142\n",
      "train loss:0.1125418882672939\n",
      "train loss:0.10264966431258538\n",
      "train loss:0.10295980394788419\n",
      "train loss:0.15915275465203316\n",
      "train loss:0.11816230410648493\n",
      "train loss:0.035405266402873094\n",
      "train loss:0.16007162939892133\n",
      "train loss:0.04495979647800255\n",
      "train loss:0.14836034264109743\n",
      "train loss:0.0864367628210961\n",
      "train loss:0.0660829889166454\n",
      "train loss:0.10565824048555998\n",
      "train loss:0.05369421125813671\n",
      "train loss:0.0939005183439332\n",
      "train loss:0.07473684240854034\n",
      "train loss:0.0667689522488503\n",
      "train loss:0.09396393065947359\n",
      "train loss:0.06583355731775002\n",
      "train loss:0.05519707081481853\n",
      "train loss:0.10675308086323172\n",
      "train loss:0.07325887549590898\n",
      "train loss:0.1925061430701768\n",
      "train loss:0.047413706137122126\n",
      "train loss:0.08229198732605665\n",
      "train loss:0.042828496239745856\n",
      "train loss:0.0790632537487646\n",
      "train loss:0.07273462578727587\n",
      "train loss:0.11029465758024326\n",
      "train loss:0.10079360611533918\n",
      "train loss:0.07105513356476592\n",
      "train loss:0.048180688366597606\n",
      "train loss:0.05156503101073289\n",
      "train loss:0.08309512824900711\n",
      "train loss:0.11493540079379118\n",
      "=== epoch:11, train acc:0.965, test acc:0.942 ===\n",
      "train loss:0.06476980103512195\n",
      "train loss:0.05157330414459151\n",
      "train loss:0.037601473667537876\n",
      "train loss:0.15137312276914375\n",
      "train loss:0.10602567224105092\n",
      "train loss:0.16248772923552438\n",
      "train loss:0.09473315231477175\n",
      "train loss:0.05049865143101487\n",
      "train loss:0.020434495228813007\n",
      "train loss:0.04657575123472036\n",
      "train loss:0.04664056098056079\n",
      "train loss:0.02149232012723249\n",
      "train loss:0.13942743312685246\n",
      "train loss:0.08210941991048654\n",
      "train loss:0.057956059893921434\n",
      "train loss:0.11837477006030862\n",
      "train loss:0.06972612663542149\n",
      "train loss:0.04019735296335382\n",
      "train loss:0.1218235586678526\n",
      "train loss:0.051638451893825515\n",
      "train loss:0.05139918205849436\n",
      "train loss:0.08864523124670248\n",
      "train loss:0.10130290979706288\n",
      "train loss:0.1771360543834108\n",
      "train loss:0.04310838981362187\n",
      "train loss:0.0650401137720878\n",
      "train loss:0.05388304528072763\n",
      "train loss:0.13989345861810773\n",
      "train loss:0.05370053179338745\n",
      "train loss:0.08504393256505838\n",
      "train loss:0.05368225596353075\n",
      "train loss:0.09592332984226762\n",
      "train loss:0.06560586885114864\n",
      "train loss:0.045586883939545304\n",
      "train loss:0.08323881637854423\n",
      "train loss:0.09492634312023597\n",
      "train loss:0.06069803676922961\n",
      "train loss:0.11741911525352805\n",
      "train loss:0.09049038559305522\n",
      "train loss:0.021671370297649872\n",
      "train loss:0.08875727363489122\n",
      "train loss:0.059302342978071326\n",
      "train loss:0.1306272189507195\n",
      "train loss:0.06543479595713601\n",
      "train loss:0.0611667693560534\n",
      "train loss:0.03957056635206669\n",
      "train loss:0.09105006406498946\n",
      "train loss:0.08917612662664798\n",
      "train loss:0.04285088357148311\n",
      "train loss:0.08597069015666799\n",
      "=== epoch:12, train acc:0.965, test acc:0.934 ===\n",
      "train loss:0.1048102335181164\n",
      "train loss:0.11731557473942787\n",
      "train loss:0.15127793790343916\n",
      "train loss:0.04093790919555771\n",
      "train loss:0.08696812861043984\n",
      "train loss:0.1548420357813589\n",
      "train loss:0.06649436947093733\n",
      "train loss:0.07574572528722992\n",
      "train loss:0.09259531773474862\n",
      "train loss:0.11264333075392174\n",
      "train loss:0.053474818715771144\n",
      "train loss:0.039711459392966414\n",
      "train loss:0.13260483533676337\n",
      "train loss:0.028947781640702074\n",
      "train loss:0.05301513765482535\n",
      "train loss:0.09831540439901412\n",
      "train loss:0.08504711045026432\n",
      "train loss:0.0759828501062332\n",
      "train loss:0.03368091637030367\n",
      "train loss:0.041946722752830186\n",
      "train loss:0.06973421687359488\n",
      "train loss:0.08450983705664926\n",
      "train loss:0.08849177099672344\n",
      "train loss:0.041296470304565264\n",
      "train loss:0.04061540632263494\n",
      "train loss:0.03160297134172023\n",
      "train loss:0.01877043437359228\n",
      "train loss:0.0984469410900165\n",
      "train loss:0.2241896773115912\n",
      "train loss:0.09054225972730254\n",
      "train loss:0.041932826658279486\n",
      "train loss:0.16484714656839455\n",
      "train loss:0.06735281385233258\n",
      "train loss:0.04759200826719653\n",
      "train loss:0.06589799236816932\n",
      "train loss:0.12459448342330556\n",
      "train loss:0.057753457462702766\n",
      "train loss:0.10335807629276428\n",
      "train loss:0.11421184592275525\n",
      "train loss:0.14744011852479055\n",
      "train loss:0.07541584705432591\n",
      "train loss:0.09583886420725454\n",
      "train loss:0.06915251021941433\n",
      "train loss:0.0775850198646017\n",
      "train loss:0.08628057667236702\n",
      "train loss:0.050164491598780975\n",
      "train loss:0.07881438722008864\n",
      "train loss:0.03502562318551892\n",
      "train loss:0.034762113739844266\n",
      "train loss:0.06851386205493291\n",
      "=== epoch:13, train acc:0.966, test acc:0.944 ===\n",
      "train loss:0.05927417317257288\n",
      "train loss:0.16227646246949604\n",
      "train loss:0.04382964406827483\n",
      "train loss:0.0423619383126034\n",
      "train loss:0.07332758315854328\n",
      "train loss:0.09760669963242816\n",
      "train loss:0.07039683249098838\n",
      "train loss:0.04603895060802778\n",
      "train loss:0.036283499642278436\n",
      "train loss:0.11629160319023002\n",
      "train loss:0.02783710266605598\n",
      "train loss:0.06326057009912729\n",
      "train loss:0.06260295083145935\n",
      "train loss:0.1193211816689697\n",
      "train loss:0.03972268121825061\n",
      "train loss:0.15159229248241154\n",
      "train loss:0.018688292543994345\n",
      "train loss:0.08200003200338352\n",
      "train loss:0.11838362094999455\n",
      "train loss:0.05272527379923065\n",
      "train loss:0.055618198026618756\n",
      "train loss:0.09891569332352555\n",
      "train loss:0.10415385074602058\n",
      "train loss:0.037214716709146\n",
      "train loss:0.0437678338447867\n",
      "train loss:0.0694155404042549\n",
      "train loss:0.027510771941228414\n",
      "train loss:0.08896217874638625\n",
      "train loss:0.02558396445432259\n",
      "train loss:0.06221531583153819\n",
      "train loss:0.05756544587720979\n",
      "train loss:0.07526374903819989\n",
      "train loss:0.09235413842867805\n",
      "train loss:0.06006170573350625\n",
      "train loss:0.04454253002462241\n",
      "train loss:0.08773390614961263\n",
      "train loss:0.03612979892610426\n",
      "train loss:0.0438501690140325\n",
      "train loss:0.024604522354022062\n",
      "train loss:0.03283292119263241\n",
      "train loss:0.05197200080952516\n",
      "train loss:0.04839030579048303\n",
      "train loss:0.04631448246658738\n",
      "train loss:0.049536078463427564\n",
      "train loss:0.08712972617587726\n",
      "train loss:0.052352585980564144\n",
      "train loss:0.04797433302901755\n",
      "train loss:0.048033393370974696\n",
      "train loss:0.08688124482848059\n",
      "train loss:0.040418206092677586\n",
      "=== epoch:14, train acc:0.981, test acc:0.952 ===\n",
      "train loss:0.055710868535618\n",
      "train loss:0.04152976282598228\n",
      "train loss:0.06558503674370857\n",
      "train loss:0.07500061700702469\n",
      "train loss:0.07424427175943982\n",
      "train loss:0.02325267826094121\n",
      "train loss:0.03866569825530198\n",
      "train loss:0.07175055494816104\n",
      "train loss:0.04560308736873913\n",
      "train loss:0.06727010416323374\n",
      "train loss:0.1475098974927776\n",
      "train loss:0.03431411629440345\n",
      "train loss:0.08508399225640172\n",
      "train loss:0.018559047253874257\n",
      "train loss:0.06182218684709298\n",
      "train loss:0.03375068144184544\n",
      "train loss:0.046137823155165354\n",
      "train loss:0.0335167230983206\n",
      "train loss:0.0340569766384169\n",
      "train loss:0.02485336799777338\n",
      "train loss:0.019015179693338234\n",
      "train loss:0.04282775313600259\n",
      "train loss:0.039413852962403746\n",
      "train loss:0.04032405627774966\n",
      "train loss:0.06498400566927347\n",
      "train loss:0.061219301763250134\n",
      "train loss:0.02872770630831315\n",
      "train loss:0.053887838939922716\n",
      "train loss:0.017608539183367963\n",
      "train loss:0.0645855158958722\n",
      "train loss:0.07078505046239651\n",
      "train loss:0.1017626443102018\n",
      "train loss:0.03910340945002067\n",
      "train loss:0.04122136275529882\n",
      "train loss:0.04231463875311515\n",
      "train loss:0.03954871868088203\n",
      "train loss:0.019038966516502047\n",
      "train loss:0.031751605216711824\n",
      "train loss:0.046213265428998354\n",
      "train loss:0.04921414439607816\n",
      "train loss:0.04444031483282195\n",
      "train loss:0.056374787303812124\n",
      "train loss:0.0138648298587454\n",
      "train loss:0.028435074005097653\n",
      "train loss:0.029545128595533824\n",
      "train loss:0.026675621173000506\n",
      "train loss:0.041290372647497214\n",
      "train loss:0.02096640016761375\n",
      "train loss:0.02032851160714869\n",
      "train loss:0.04847663349205065\n",
      "=== epoch:15, train acc:0.984, test acc:0.949 ===\n",
      "train loss:0.02491170687668504\n",
      "train loss:0.0475691848904617\n",
      "train loss:0.060488032392339935\n",
      "train loss:0.03664630194314779\n",
      "train loss:0.0654433275036689\n",
      "train loss:0.01395979490642997\n",
      "train loss:0.022596542047963363\n",
      "train loss:0.03806036663805056\n",
      "train loss:0.01623959578126463\n",
      "train loss:0.03795321082047107\n",
      "train loss:0.028180474259841427\n",
      "train loss:0.024863535814330585\n",
      "train loss:0.11330657565211644\n",
      "train loss:0.06045931996957259\n",
      "train loss:0.04598035706344501\n",
      "train loss:0.0254731973620059\n",
      "train loss:0.013803653666699495\n",
      "train loss:0.02765472597944478\n",
      "train loss:0.07241917881200355\n",
      "train loss:0.03479431232801123\n",
      "train loss:0.022173707199676415\n",
      "train loss:0.05435437044381525\n",
      "train loss:0.07802538264192353\n",
      "train loss:0.03919929590987987\n",
      "train loss:0.0750329112056593\n",
      "train loss:0.03226373567491954\n",
      "train loss:0.044484081181422594\n",
      "train loss:0.027043768356299944\n",
      "train loss:0.022992502306756174\n",
      "train loss:0.02018380095112772\n",
      "train loss:0.041882100188029125\n",
      "train loss:0.03439173611668273\n",
      "train loss:0.02802152867837866\n",
      "train loss:0.05520987789321314\n",
      "train loss:0.04444895414474012\n",
      "train loss:0.015734780855973386\n",
      "train loss:0.02347547205119647\n",
      "train loss:0.10723155881197834\n",
      "train loss:0.08033328035375818\n",
      "train loss:0.029583872215751778\n",
      "train loss:0.07636227060983665\n",
      "train loss:0.03446298408027898\n",
      "train loss:0.01809746269521427\n",
      "train loss:0.026352727532594863\n",
      "train loss:0.027892236864618628\n",
      "train loss:0.018672811768581817\n",
      "train loss:0.04226409150630896\n",
      "train loss:0.023693412349541868\n",
      "train loss:0.024089332700120078\n",
      "train loss:0.03961391703079648\n",
      "=== epoch:16, train acc:0.976, test acc:0.948 ===\n",
      "train loss:0.03995022570978349\n",
      "train loss:0.03699181417838269\n",
      "train loss:0.03245833172667952\n",
      "train loss:0.022290507299990065\n",
      "train loss:0.01838574386164489\n",
      "train loss:0.035050912554244064\n",
      "train loss:0.02237838517267119\n",
      "train loss:0.025025603728453093\n",
      "train loss:0.019561289662517186\n",
      "train loss:0.03474403640973569\n",
      "train loss:0.07772463736844376\n",
      "train loss:0.034222865809393196\n",
      "train loss:0.023864520521768226\n",
      "train loss:0.02150412415568982\n",
      "train loss:0.028550376972057402\n",
      "train loss:0.04728162346968474\n",
      "train loss:0.029217931521799984\n",
      "train loss:0.08262326444791297\n",
      "train loss:0.08196054263270572\n",
      "train loss:0.027013103896930263\n",
      "train loss:0.04821435644473062\n",
      "train loss:0.059194303397248946\n",
      "train loss:0.027606719404763752\n",
      "train loss:0.023066267917246145\n",
      "train loss:0.012660447976911572\n",
      "train loss:0.03422430533719577\n",
      "train loss:0.0573130881568928\n",
      "train loss:0.04652950474300029\n",
      "train loss:0.01764221101741495\n",
      "train loss:0.07051724826835878\n",
      "train loss:0.030782746752994496\n",
      "train loss:0.01724751088804467\n",
      "train loss:0.01518226128429119\n",
      "train loss:0.03510497526887833\n",
      "train loss:0.04226592870382071\n",
      "train loss:0.021972102948210864\n",
      "train loss:0.030399653952651005\n",
      "train loss:0.03308308924812892\n",
      "train loss:0.07607903002588674\n",
      "train loss:0.024794581395623707\n",
      "train loss:0.021229146500234348\n",
      "train loss:0.04647620072207839\n",
      "train loss:0.012575926999858685\n",
      "train loss:0.045553444725624274\n",
      "train loss:0.028476778245555997\n",
      "train loss:0.08534781937828423\n",
      "train loss:0.02896598672173399\n",
      "train loss:0.11331092793850628\n",
      "train loss:0.03229661531169427\n",
      "train loss:0.02082037278619615\n",
      "=== epoch:17, train acc:0.988, test acc:0.956 ===\n",
      "train loss:0.02640836828502124\n",
      "train loss:0.04648305981581127\n",
      "train loss:0.012304436506037124\n",
      "train loss:0.016243779430590886\n",
      "train loss:0.018010842211839115\n",
      "train loss:0.013073561169965648\n",
      "train loss:0.033288934865840704\n",
      "train loss:0.11552009006986955\n",
      "train loss:0.014771821516271832\n",
      "train loss:0.1048185424427084\n",
      "train loss:0.020904813376670224\n",
      "train loss:0.018003792566346676\n",
      "train loss:0.017988772483765708\n",
      "train loss:0.02240356423069592\n",
      "train loss:0.03943957627040585\n",
      "train loss:0.01873138131783933\n",
      "train loss:0.028016915393803864\n",
      "train loss:0.04655149492218676\n",
      "train loss:0.020290613327516173\n",
      "train loss:0.012311460966956643\n",
      "train loss:0.03324198213050103\n",
      "train loss:0.01343901102797417\n",
      "train loss:0.0146836130063467\n",
      "train loss:0.041168524378534715\n",
      "train loss:0.01664423245343189\n",
      "train loss:0.09172818659766255\n",
      "train loss:0.02010571118532163\n",
      "train loss:0.03358993917130246\n",
      "train loss:0.012005409579340275\n",
      "train loss:0.03758279080481817\n",
      "train loss:0.016551906665067555\n",
      "train loss:0.012366981817871643\n",
      "train loss:0.010615539200029285\n",
      "train loss:0.01196688597711411\n",
      "train loss:0.022451955863735603\n",
      "train loss:0.02089817297606887\n",
      "train loss:0.0791158670777275\n",
      "train loss:0.020197399147552093\n",
      "train loss:0.026182731619871332\n",
      "train loss:0.019983348117314767\n",
      "train loss:0.029036766005880656\n",
      "train loss:0.03843339483899021\n",
      "train loss:0.02753650116831785\n",
      "train loss:0.03216084420316407\n",
      "train loss:0.04391462280667434\n",
      "train loss:0.021281702912592553\n",
      "train loss:0.014179290078395548\n",
      "train loss:0.03766743478245254\n",
      "train loss:0.06410886527874154\n",
      "train loss:0.020764826846014276\n",
      "=== epoch:18, train acc:0.983, test acc:0.952 ===\n",
      "train loss:0.0359268193943377\n",
      "train loss:0.04284095811040722\n",
      "train loss:0.10080905828211022\n",
      "train loss:0.03127609512810277\n",
      "train loss:0.028440257760618717\n",
      "train loss:0.0170087628828517\n",
      "train loss:0.021851937668637397\n",
      "train loss:0.028065265357301437\n",
      "train loss:0.018434323186584504\n",
      "train loss:0.03014295528345536\n",
      "train loss:0.055517265673638946\n",
      "train loss:0.012199690226239146\n",
      "train loss:0.022518447706151492\n",
      "train loss:0.020322218960779993\n",
      "train loss:0.014450652330572508\n",
      "train loss:0.014180038884171114\n",
      "train loss:0.044073865538483734\n",
      "train loss:0.026827050841817832\n",
      "train loss:0.030667926791209908\n",
      "train loss:0.03410448796868222\n",
      "train loss:0.00839697653442542\n",
      "train loss:0.07406743155475423\n",
      "train loss:0.011902775696004453\n",
      "train loss:0.014967752818169202\n",
      "train loss:0.021435188530545924\n",
      "train loss:0.006395379621942893\n",
      "train loss:0.028585643859011533\n",
      "train loss:0.018515901644056736\n",
      "train loss:0.03434753896827319\n",
      "train loss:0.018433169300494795\n",
      "train loss:0.014858488359216087\n",
      "train loss:0.01550884991983393\n",
      "train loss:0.008860682907142257\n",
      "train loss:0.019742032417808874\n",
      "train loss:0.016160335608015983\n",
      "train loss:0.016930220620770535\n",
      "train loss:0.006060791933975934\n",
      "train loss:0.06330903937190262\n",
      "train loss:0.04577787517639832\n",
      "train loss:0.03810265991798079\n",
      "train loss:0.007817620857652384\n",
      "train loss:0.017174712350455305\n",
      "train loss:0.033843181872481864\n",
      "train loss:0.011194454052623377\n",
      "train loss:0.02328493483239716\n",
      "train loss:0.02221247618998951\n",
      "train loss:0.03265640775525134\n",
      "train loss:0.02680302469354345\n",
      "train loss:0.016727923281884202\n",
      "train loss:0.0202546768764899\n",
      "=== epoch:19, train acc:0.994, test acc:0.953 ===\n",
      "train loss:0.009089717597700242\n",
      "train loss:0.0111081373948175\n",
      "train loss:0.0205238767492532\n",
      "train loss:0.024936018547264665\n",
      "train loss:0.021709194153417318\n",
      "train loss:0.03690156896087242\n",
      "train loss:0.025057076864093107\n",
      "train loss:0.013799292668299936\n",
      "train loss:0.009545073367249928\n",
      "train loss:0.015350944266094253\n",
      "train loss:0.023123939099629346\n",
      "train loss:0.012980448046508435\n",
      "train loss:0.010202769459895115\n",
      "train loss:0.030900380879333122\n",
      "train loss:0.03417129068147377\n",
      "train loss:0.022106836256715435\n",
      "train loss:0.017029193590090978\n",
      "train loss:0.02181363491162318\n",
      "train loss:0.02278043193292995\n",
      "train loss:0.023615835505232344\n",
      "train loss:0.017845152351325323\n",
      "train loss:0.016070819436705554\n",
      "train loss:0.026944779400915504\n",
      "train loss:0.01873054175043451\n",
      "train loss:0.03219336217125625\n",
      "train loss:0.018159495875008563\n",
      "train loss:0.018612199601041567\n",
      "train loss:0.046411213433753803\n",
      "train loss:0.009735275446653775\n",
      "train loss:0.01529730418239255\n",
      "train loss:0.011178121430369054\n",
      "train loss:0.01611694164490183\n",
      "train loss:0.014250543776631684\n",
      "train loss:0.0828001432105399\n",
      "train loss:0.011461211099039635\n",
      "train loss:0.04525638014661602\n",
      "train loss:0.04041100376157143\n",
      "train loss:0.010468516624234798\n",
      "train loss:0.0077213268455667726\n",
      "train loss:0.012431535357360112\n",
      "train loss:0.026984036829996613\n",
      "train loss:0.01706777856771101\n",
      "train loss:0.025183949931797248\n",
      "train loss:0.018293960476844597\n",
      "train loss:0.025927215163962566\n",
      "train loss:0.007010742302129359\n",
      "train loss:0.048657961318834875\n",
      "train loss:0.047065465066201725\n",
      "train loss:0.04706038528612114\n",
      "train loss:0.0071709246788919405\n",
      "=== epoch:20, train acc:0.994, test acc:0.957 ===\n",
      "train loss:0.029746140335477553\n",
      "train loss:0.04182233380888491\n",
      "train loss:0.017282439444906598\n",
      "train loss:0.022571810203681528\n",
      "train loss:0.01092824882377664\n",
      "train loss:0.03910304991035077\n",
      "train loss:0.028054776867249852\n",
      "train loss:0.020289501128714683\n",
      "train loss:0.07248415530397305\n",
      "train loss:0.009327298116831001\n",
      "train loss:0.02139637754199035\n",
      "train loss:0.02473326203717658\n",
      "train loss:0.005809112408350811\n",
      "train loss:0.01948994769714426\n",
      "train loss:0.007717134762057915\n",
      "train loss:0.018238998981950375\n",
      "train loss:0.033671831317087696\n",
      "train loss:0.008026291827505682\n",
      "train loss:0.012550744277059609\n",
      "train loss:0.02360484480304381\n",
      "train loss:0.017853510163878303\n",
      "train loss:0.013388954257525445\n",
      "train loss:0.005358343794723711\n",
      "train loss:0.006617514504956544\n",
      "train loss:0.023358321867931436\n",
      "train loss:0.006737738109881802\n",
      "train loss:0.01933906608690258\n",
      "train loss:0.013321626583725073\n",
      "train loss:0.009395115801306501\n",
      "train loss:0.008105514444307453\n",
      "train loss:0.010655433016865008\n",
      "train loss:0.014935670216567904\n",
      "train loss:0.04948523417770961\n",
      "train loss:0.012007892565775467\n",
      "train loss:0.013694270591723007\n",
      "train loss:0.023964505830565433\n",
      "train loss:0.028118495320399492\n",
      "train loss:0.026157957589333158\n",
      "train loss:0.021456148679702816\n",
      "train loss:0.006998917898708969\n",
      "train loss:0.021257511979611173\n",
      "train loss:0.021894396716150406\n",
      "train loss:0.02385315871398217\n",
      "train loss:0.019889279719821965\n",
      "train loss:0.017264559664695145\n",
      "train loss:0.020889224819641786\n",
      "train loss:0.010591508479493245\n",
      "train loss:0.018874047482883315\n",
      "train loss:0.006502145837302701\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.958\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXQc1Zn38e+j1r5YsiXZlmWDV2xMABuMgbBkB0wImFkyIQPJkJk4TCCTzAQP5s0EyCQ548RJJocJgWEyQHZCWAwTzBKWkIUYb3jBxsbyhiRblrxIsmSt3ff9o1p2q93dai2llt2/zzl9uqvqVtfTZfk+Vbdu3TLnHCIikr4yUh2AiIiklhKBiEiaUyIQEUlzSgQiImlOiUBEJM0pEYiIpDnfEoGZPWRm9Wb2VpzlZmb3mlmVmW00s/P8ikVEROLz84zgEeCqBMsXADPCr0XA/T7GIiIicfiWCJxzvwcOJShyHfAT51kJlJhZhV/xiIhIbJkp3HYlUB0xXROety+6oJktwjtroKCg4PxZs2YNS4AicmpoPNpFXXM7XcEQWYEMxo/KpSQ/a9i2XdvYRihiFAczqCjOozivfzFkGGSYDSiOtWvXHnDOlcdalspEEOvXxBzvwjn3IPAgwLx589yaNWv8jEtEhtjyN2tZ9sI29ja2MaEkj8VXzmTh3Mph2/adT26irCt4bF5WVoB/+4uzhzyGju4g7x48ys4Drew60MquhlaeerOWccHQCWUd0NjP77/lfdNYsmBgB8JmtifeslQmghpgUsT0RGBvimIREZ/0VMRt4Yq4trGNO5/cBOBbMnDOsb+5g6r6Fu5+5q1j2+7R1hXk35a/RV1zO8V5WZTkZVGcl8WovCxK8r3PhTmZWIyj71DIsa+5nV0Nrew80MLOhnClf6CVmsNHCUUczpYV5tAZIwn0+Pp1Z/Xrd82eUNyv8slKZSJ4BrjNzB4FLgSanHMnNAuJSOq1dHSzra6ZxqNdtHUFOdoZpL0rSFvn8c9HO4O0hef1vB/tCrK5tonuUO+T/bauIF992qugxxfnMn5ULhXFuRTnZcWsfOMJhRy1jW1srz9CVX0L2/e3sL2+hR31LRzp6O7zNy19bmvc5YEMozgiQYzKzaThSAe7DrTS0X28cs/PDjClrIBzJ5WwcG4lU8sKmFJWwOSyAorzsrhk6SvUNrad8P2VJXncdPHkpH+rn3xLBGb2S+D9QJmZ1QB3A1kAzrkHgBXA1UAVcBS42a9YRCR5bZ1BtuxrZlNNIxtrmthY28SOhhYSDVScm5VBXlaA/OxM73N2gPysTIrzsk5IAj2OtHcfOzOI/J6K4jzGj8r1EkSxlyDGj8qlvCiH/c0d7GhoYfv+I16F39BCe9fxSrm8KIfp5YVcf14lM8YWMm1sIf/yqw3UNbefsP3Kklx++y/vo/FoF01t3qvxaBfNbV00tnX2mtfU1kVzezeVJXlcOr2MKeUFTC0rZGp5AWOLchImr8VXzux1RgSQlxVg8ZUz4+/QYeZbInDO3dDHcgfc6tf2RUaSVLaRJ9p+R3eQrfuOsLG26VjFv72+hWC48i4vyuHcicV87JwJvKdyFGWFOeRlB8jLCniVfXaA3MwAGRnxK8J4R8QTSnJ5/Jb3sq+pnbqmdvY1tVHX1E5dsze9atch9je3x0wkE4pzmT6uiIumljJ9bCEzxhYyfWwhJfnZJ5RdsmBWnIp4FvnZmeRnZzKhJG8guzUpPf/Oqfz374udbM8j0MViOdlEt5GDVxH9hw8XK5PdfiDDqBiVw/4jHXQFvTpgTEE2Z1cWc87EYs6uLObcSSWMG5Xry/aT/f2hkONAawf7mzrY39xOeVEO08YWUpjTv2PYVCfikcDM1jrn5sVcpkQg0j9dwRAt7d20dES82rs5En5v6ejqNf1/G/f2ar7okRUwzurHxT+HVzEGe14u4nOCedEXSnvkZGbwmUuncE5lMWdPLKayJK9f7fP9kdKKeNkMaK0/cX7BWFi8fXhiGAESJYJUXiwWGTb9qYicczS0eD1Oei5AVtW3sPNAC41Hu3pdKIzHDAqzMynMzYyZBAC6go5R/exHHjAIZGQQyPCO6gMZGVHzvPfMjAwyzAhkwP/8YVfM7+rsDnHHVcNwT86yGSxsrWchQC7QDjwNvDRMFXGsJJBo/lAbqkTU3QEuBFlD34ylRCCnvHjdF51zzJ9ayvb9R45X+uH3prauY+sX5WYyfWwhl80op7Qgm8Icr4IvzMmkKDeTwpwsCnICxz4X5maSn3W83TxRr5GffGa+779/xaa6OG30/rWL9zKcFXFnKzTvheZaaN7nvSfy3B2QUxTxGuW9ZxeeOC8z6vpDKAQuCKFuCIXfXShqOpj496+8HzpaoKMZOo70fnVGTQc74bIvw4fuGpp9FUGJQE5qoZCjMxjyXt0Rr/B0R3eIbzy7JWY/8n95bEOvOxjHFGQzfWwh15xTEb74WMSMcYV99grpy8vuH8jNPXjC/HZXCuwc8PcO6fadg7bD4Uq0pyKN+Hxkn/eyAGTlQ3a+d2SalR9+5UF2QXhez3t4XiJ7XoeMTO97M3pecaYtA44eiIgrKs4je6G9qX87Z/0vvQrX9X2WRyDbi6Gnoo99/2v/PL/Ee8/MCyedwuPJp3hSVJIqgkkXDX6bMSgRyLDobxtxc3uXd5S+v4WqcHfB3QeP0trR3avSj9c1MRkO+MbC9xzrcVJamDPg70okt+PESjjR/GHd/iPXHK9Mu6O6WFoGFI6HURVQNgMmX+bN7zrqvTrD7+1NXpI4Nq8NulqTq1wfXjCIX2ZQOM6Lr3QaTLkMRk2AUZXee1GF9/7N8fG/4s53vSTYdTTi6DvG0XlHs3fkjgsnqczjiarXdKa33yKnl/9j/O3/6y6vgg8Mz3AX8SgRiO8S3Vl6+RnlXtNMw/G2+Kr6ll79vrMzM5hWXsjsilEU5mSSnZlx/BXw3nOipiM/3/7rDRxo6TwhrsqSPG686PTEwQe7oGEb1G2E+re9Su5Yc0Do+Ol/z1FirKaBRHb93qu4iiq8o+yBCgW9yvzwLji8Gw7t8j4fin194Ph63TBhLsz6KBRN6F2RFo6DwACrCOe8pozOVvj2lPjlbloe3n/B3vvshOnwPs0vjajoxw9NBWrmnblkF3jfOdQSJYL8MUO/vQFQIhDffev5rXGaZtb3uh0/PzvA9LGFvHd6KTPGFh3rHz5pTD6BBP3U+/LHwOeSa5rpaIH9b8G+jV7F31P5B8NJJJDjVdbRTRd9HREm8uOPHf+cNzqiMo6okHteBWO9duXoiv7wbmjcczxO8LZbchqMTlAJA3zm+cTLB8oMMnO8VyLTPuDP9iP17LdY8wVQIpBBcM7R3NZNXfPxm4F6bg7quSloX1MbL7vPUp57Ytttgyvm6Q//zqvwxxVRMSo34Y1JA5WwaeQP34W6TV7lf2gnx9p980th/Dlw4S1Qca73uXSaV7n31z0Juoh+6hmvWSW6TX7fhr4vpuaMgtGTYdxsmHW1V+mPmeK9j6o8fjSfaPvDIdUVcaq7iKb69ydBiUD61BUMsXZHHTXrX+ZgSxv1Rx37Wx37WkI0dwfoJJNOl0UnmXRZFkUFBZQVF3J6aT4XTh1D+ZuxL+CVWxP/cBbQsQ8a34H90e2yUW20na3ekaYF4h99xzpST+Tlf4eS02H82XDuJ7wKf/zZ3hG4T33qe5n6vvjLujvDSSKcHFoboKDcq+hHT/aaFYYjxsFKdUWcaifB71cikJgaj3by2jsNrN+wjtN2PcZ17hUuspbehQLhV7Ru4CBwOKvvpoF758ZfZoGoLn3hNvRjbfDBqO570dPdfV+wvGMP5JUkLjNYAz0izMyG0ad7r1RsX9KGEoEAXjPPjoYWXn67nt9t2UtJzUvckPEydwc2ESRAw8QP0/bevyOvqBSCHd7RarDDu8kl2Bn1Hrm8E1beF3/DCx+I3Y87p8jrfjgUR7yJmkb8TgKQ+iPCVG9fRjwlgjQQr+tmZ3eIVbsO8fLW/byytZ7Og9V8IvNV/ivrd5RlHaKzYAKhC75C4LybGD9qEE8RTZQI5iQcm1BEhoESwSkuVtfNxY9v4OE/7WRHw1FaOzr5YNZbfL/wNc7NfQPDYdM/AvM+Q/aMKwZ2cXSkUdOISEJKBKeYUMhxsLXzWI+dS5++mLcDTSe05R9oGMWaihu4/Miz5LfWQEY5XPbPcN6nB98mHS3VFbGaRkQSUiI4iXQHQzS0dESM395OXVMb+5ra2d/cfuy9Z1hhgN0xum0ClFkzV9X9t3e36IKvw6xrThxLZaioIhYZ0ZQITgIbqht5+E+7WLGp7oTnn/Y80WncqBwumDzm2BOdJuSHOM3theUJvvjW1VB+hr/Bi8iIp0QwQnUFQ7ywuY6H/riLde82UpiTyd9cMInZE0Yde4xfRWGA4o592KEdcHAzHNgOdVWweYc3AFdflAREBCWCEedwaye/XP0uP/3zHvY1tXN6aT53f2w2fz3DKKx+DQ68A1U74OB2b2iBUMQDuvPGQOl0mPp+7y7Yshnw2KdS9EtE5GShRDBCvLP/CA//aRdPvVlLe1eIS6aX8p0Pj+LijtfJ2PJ1+O1ar2BmLoyZBmNnw+zrvIq/5zVCBrASkZOLEkEKhUKOV7fV8/CfdvPHqgPkZGbwudnd3FS8kfLq5+BZb4ROKuZ4D6OYdQ2UzoCMjOQ3kuoeOyIy4ikRDIPoG7q+8MHptHUF+fHru9l9sJVLCuv59cwtzG39A5nvbPVWmjgfrvgGnHnt4LpzqseOiPRBicBn7f8xlYUdB3s/r3UFHHaFjCpYwJWlqyhs3QN7DE6/BC74tnfkXzxMD/YWkbSnROCzeEMgj7YW/rLtSZhyOcz+klf5F6q5RkSGnxKBj4IhF3NwzmMWV+kCr4ikXD+uOkp/rK9u5OP3vZq4kJKAiIwAOiMYYgdbOlj2wjZq167gP7MfSXU4IiJ9UiIYIsGQ4xdv7OHHL6zkn4MP89HslYRGT4XDqY5MRCQxJYIhsHbPYe5ZvoEL6h/nmewnyM0OwuVfIeO9/wTfP1v9+EVkRFMiGIQDLR0sfW4rO9e9wndzH+GMrN24aR/Brv42jJnqFVI/fhEZ4ZQIBqA7GOJnK/fwo9+u5Quhn/GdnFcJFU6ABT/FzvzYyfFAcRGRMCWCflq9+xB3PbWR9xx4ludyHqXQWuHiL5DxviWQU5jq8ERE+k2JIEntXUH+35Ob2LL+z3wr9xHOzdqKm3gx9tHvwrizUh2eiMiAKREk6fmN1Zy56Vt8J+cFLLcErvghdu4N/RsATkRkBFIiSNbm5Xw2cwWhOTdiV3xdN4OJyCnD18NZM7vKzLaZWZWZLYmxvNjM/s/MNpjZZjO72c94BqOofg1HySPj2nuVBETklOJbIjCzAHAfsACYDdxgZrOjit0KbHHOnQu8H/iumfn0BPXBmdjyFtUFsyEj4ehBIiInHT/PCOYDVc65nc65TuBR4LqoMg4oMjMDCoFDQDcjzOHDh5judtNSfl6qQxERGXJ+JoJKoDpiuiY8L9IPgDOBvcAm4IvOuVD0F5nZIjNbY2ZrGhoa/Io3rr1bXidgjuzJFw37tkVE/OZnIoh1V5WLmr4SWA9MAOYAPzCzUSes5NyDzrl5zrl55eXlQx9pH9p3/RmA8bMvHfZti4j4zc9EUANMipieiHfkH+lm4EnnqQJ2AbN8jGlA8vevZSeVlJWPS3UoIiJDzs9EsBqYYWZTwheAPwE8E1XmXeBDAGY2DpgJ7PQxpv5zjsqWzbybdxamoSNE5BTk230EzrluM7sNeAEIAA855zab2S3h5Q8AXwceMbNNeE1JdzjnDvgV00CEDlQxyjXTrAvFInKK8vWGMufcCmBF1LwHIj7vBa7wM4bBOrTtj5QBmadfmOpQRER8ofER+tC28880u3wmzJiT6lBERHyhRNCH3Lp1rHfTOGP8CZ2ZREROCUoEibQ3U3q0ih05s8nP1rBMInJqUiJIpHYtGTiay+amOhIREd8oESTQtecNQs7IPG1+qkMREfGN2jsSaN+5kr2ukikTJ6Q6FBER3+iMIJ5QiJz9a1kXmsGs8UWpjkZExDdKBPEcrCK7q5lNGWdwemlBqqMREfGNEkE8NasAODx6DoEMDS0hIqcuJYJ4qt+gmQKKKs9MdSQiIr5SIoije88q1gWnc0ZFcapDERHxlRJBLO1NBA5uY21oBrN0R7GInOKUCGKpWYPhWOdmMKtCPYZE5NSmRBBLzWpCGDV5Z1JWmJPqaEREfKVEEEv1Kt4NnM7ECj2RTEROfUoE0UIhXM1q3uiapusDIpIWlAiiHdiGdTSzOjidmbqjWETSgBJBtGrvRrK1oTM0tISIpAUlgmg1q2jLLGYP45kxVolARE59Gn00WvVqtmfPYnJuIXnZgVRHIyLiO50RRGo7DAe2sapL1wdEJH0oEUSqWQPAK62T1WNIRNKGEkGk6lU4y2B9aJrOCEQkbSgRRKpZRWPRDI6Sqx5DIpI2lAh6hIJQs5aqnNnkZQU4bUx+qiMSERkWSgQ9GrZC5xFWdU3njPFFZOhhNCKSJpQIeoRvJHux+TRmjVOzkIikDyWCHjWrCeWVsuHoGA09LSJpRYmgR/UbHB5zLmDqMSQiaUWJAODoIThYxY6c2QC6h0BE0ooSAUDNagBWB89gbFEOYwqyUxyQiMjwUSIA70KxBXi5eYKahUQk7SgRANSswo17D281dOtGMhFJO0oEwW6oXUdT2Vw6u0O6PiAiacfXRGBmV5nZNjOrMrMlccq838zWm9lmM3vNz3hiqt8CnS3syvUuFKtpSETSjW/PIzCzAHAf8BGgBlhtZs8457ZElCkBfghc5Zx718zG+hVPXDU9TySbQSCjneljC4c9BBGRVPLzjGA+UOWc2+mc6wQeBa6LKvNJ4Enn3LsAzrl6H+OJrXo1FJTzxuEippQVkJulh9GISHrxMxFUAtUR0zXheZHOAEab2e/MbK2ZfSrWF5nZIjNbY2ZrGhoahjbKmlUw6UK27j+iZiERSUt+JoJYo7a5qOlM4Hzgo8CVwFfN7IwTVnLuQefcPOfcvPLy8qGLsPUAHNpJR8X5VB9q0xhDIpKWkkoEZvaEmX3UzPqTOGqASRHTE4G9Mco875xrdc4dAH4PnNuPbQxO+Eay3XlnATCrQj2GRCT9JFux34/Xnr/dzJaa2awk1lkNzDCzKWaWDXwCeCaqzNPAZWaWaWb5wIXA20nGNHjVb0BGJuu7pwDoHgIRSUtJ9Rpyzr0EvGRmxcANwG/NrBr4H+BnzrmuGOt0m9ltwAtAAHjIObfZzG4JL3/AOfe2mT0PbARCwI+cc28NyS9LRvVqGH82Wxo6KcgOUFmSN2ybFhEZKZLuPmpmpcCNwE3Am8DPgUuBTwPvj7WOc24FsCJq3gNR08uAZf0JekgEu2HvOph7E1urvQvFehiNiKSjZK8RPAn8AcgHPuacu9Y59yvn3BeAk7Pj/f63oOsobuIFbK07wkzdUSwiaSrZM4IfOOdeibXAOTdvCOMZPuELxQ2j59DU9o6uD4hI2kr2YvGZ4buAATCz0Wb2eZ9iGh7Vq6BwPFtavTMBJQIRSVfJJoLPOucaeyacc4eBz/oT0jCpWQWTLmDr/hZAD6MRkfSVbCLIMLNjV1LD4widvE9vaamHw7th4ny21R1h/KhcivOzUh2ViEhKJJsIXgAeM7MPmdkHgV8Cz/sXls+qvYHmmDSfrXVH9LB6EUlrySaCO4BXgH8EbgVeBv7Vr6B8V7MKMrLoGncOVfUaY0hE0luyN5SF8O4uvt/fcIZJ9WqoOJddjUG6gk4XikUkrSV7H8EMM3vczLaY2c6el9/B+SLYBXvfPNYsBLpQLCLpLdmmoYfxzga6gQ8APwF+6ldQvqrbBN1tMPECtu5rJjPDmFZ+ct4TJyIyFJJNBHnOuZcBc87tcc7dA3zQv7B8FHGheFvdEaaWF5CdqUc3i0j6SvbO4vbwENTbwwPJ1QLD/1jJoVCzCoomQPFEtta9w/mnj051RCIiKZXsofCX8MYZ+ie8B8nciDfY3MmnejVMuoDm9i5qG9vUY0hE0l6fiSB889jHnXMtzrka59zNzrm/dM6tHIb4htaROmh6FybO551jF4qVCEQkvfWZCJxzQeD8yDuLT1rHrg9ceLzHkJ5KJiJpLtlrBG8CT5vZr4HWnpnOuSd9icovNasgkA0V57B17TsU5WYyoTg31VGJiKRUsolgDHCQ3j2FHDDyE8GyGdBa33veN8by5YzRbB33C06FEx0RkcFI9s7im/0OxDfRSSBsdOiwxhgSESHJRGBmD+OdAfTinPvMkEc0jPRUMhGR5JuGfhPxORe4Htg79OEML/UYEhFJvmnoichpM/sl8JIvEQ0j3UMgIpL8DWXRZgCnDWUgqTAqVw+jERFJ9hrBEXpfI6jDe0bByFcwNuYF46aM0RSnIBwRkZEm2aahk7cNZfH2XpOd3SFm3/U8iy6fehI/WUdEZOgk+zyC682sOGK6xMwW+heWf3Y0tNAdcro+ICISluw1grudc009E865RuBuf0Ly1zY9jEZEpJdkE0Gscsl2PR1RttYdIStgTC0vSHUoIiIjQrKJYI2Zfc/MppnZVDP7T2Ctn4H5ZWtdM9PKC8kK6GE0IiKQfCL4AtAJ/Ap4DGgDbvUrKD9tqzuiG8lERCIk22uoFVjicyy+azraxb6mdg09LSISIdleQ781s5KI6dFm9oJ/Yflja10zoDuKRUQiJds0VBbuKQSAc+4wJ+Ezi7ft11PJRESiJZsIQmZ2bEgJM5tMjNFIR7qtdUcozsti/Cg9jEZEpEeyXUC/AvzRzF4LT18OLPInJP9s3dfMzPFFehiNiEiEpM4InHPPA/OAbXg9h76M13PopLD8zVouWfoy695tZHNtE8vfrE11SCIiI0ayF4v/AXgZLwF8GfgpcE8S611lZtvMrMrM4vY6MrMLzCxoZn+VXNjJW/5mLXc+uYnaxnYAWjuD3PnkJiUDEZGwZK8RfBG4ANjjnPsAMBdoSLSCmQWA+4AFwGzgBjObHafctwBfeiEte2EbbV3BXvPauoIse2GbH5sTETnpJJsI2p1z7QBmluOc2wrM7GOd+UCVc26nc64TeBS4Lka5LwBPALEfLjxIextjt2DFmy8ikm6STQQ14fsIlgO/NbOn6ftRlZVAdeR3hOcdY2aVeI+9fCDRF5nZIjNbY2ZrGhoSnoicYEJJXr/mi4ikm2QvFl/vnGt0zt0DfBX4X6CvYahjdc2J7nL6feAO51wwRtnI7T/onJvnnJtXXl6eTMjHLL5yJnlZgV7z8rICLL6yrxMaEZH00O8RRJ1zr/VdCvDOACZFTE/kxLOIecCj4e6cZcDVZtbtnFve37jiWTjXOwlZ9sI29ja2MaEkj8VXzjw2X0Qk3fk5lPRqYIaZTQFqgU8An4ws4Jyb0vPZzB4BfjOUSaDHwrmVqvhFROLwLRE457rN7Da83kAB4CHn3GYzuyW8POF1ARERGR6+PlzGObcCWBE1L2YCcM79nZ+xiIhIbHo6i4hImlMiEBFJc0oEIiJpTolARCTNKRGIiKQ5JQIRkTSnRCAikuaUCERE0pwSgYhImlMiEBFJc0oEIiJpTolARCTNKRGIiKQ5JQIRkTSnRCAikuaUCERE0pwSgYhImlMiEBFJc0oEIiJpTolARCTNKRGIiKQ5JQIRkTSnRCAikuaUCERE0pwSgYhImlMiEBFJc0oEIiJpTolARCTNKRGIiKQ5JQIRkTSnRCAikuaUCERE0pwSgYhImlMiEBFJc74mAjO7ysy2mVmVmS2JsfxvzWxj+PW6mZ3rZzwiInIi3xKBmQWA+4AFwGzgBjObHVVsF/A+59w5wNeBB/2KR0REYvPzjGA+UOWc2+mc6wQeBa6LLOCce905dzg8uRKY6GM8IiISg5+JoBKojpiuCc+L5++B52ItMLNFZrbGzNY0NDQMYYgiIuJnIrAY81zMgmYfwEsEd8Ra7px70Dk3zzk3r7y8fAhDFBGRTB+/uwaYFDE9EdgbXcjMzgF+BCxwzh30MR4REYnBzzOC1cAMM5tiZtnAJ4BnIguY2WnAk8BNzrl3fIxFRETi8O2MwDnXbWa3AS8AAeAh59xmM7slvPwB4C6gFPihmQF0O+fm+RWTiIicyJyL2Ww/Ys2bN8+tWbMm1WGIiJxUzGxtvANtP68RiIiMGF1dXdTU1NDe3p7qUHyVm5vLxIkTycrKSnodJQIRSQs1NTUUFRUxefJkwk3RpxznHAcPHqSmpoYpU6YkvZ7GGhKRtNDe3k5paekpmwQAzIzS0tJ+n/UoEYhI2jiVk0CPgfxGJQIRkTSnRCAiEsPyN2u5ZOkrTFnyLJcsfYXlb9YO6vsaGxv54Q9/2O/1rr76ahobGwe17b4oEYiIRFn+Zi13PrmJ2sY2HFDb2MadT24aVDKIlwiCwWDC9VasWEFJScmAt5sM9RoSkbTztf/bzJa9zXGXv/luI53BUK95bV1B/vXxjfxy1bsx15k9YRR3f+ysuN+5ZMkSduzYwZw5c8jKyqKwsJCKigrWr1/Pli1bWLhwIdXV1bS3t/PFL36RRYsWATB58mTWrFlDS0sLCxYs4NJLL+X111+nsrKSp59+mry8vAHsgd50RiAiEiU6CfQ1PxlLly5l2rRprF+/nmXLlrFq1Sq++c1vsmXLFgAeeugh1q5dy5o1a7j33ns5ePDEode2b9/OrbfeyubNmykpKeGJJ54YcDyRdEYgImkn0ZE7wCVLX6G2se2E+ZUlefzqcxcPSQzz58/v1df/3nvv5amnngKgurqa7du3U1pa2mudKVOmMGfOHADOP/98du/ePSSx6IxARCTK4itnkpcV6DUvLyvA4itnDtk2CgoKjn3+3e9+x0svvcSf//xnNmzYwNy5c2PeC5CTk3PscyAQoLu7e0hi0RmBiEiUhXO9Z2gte2EbexvbmFCSx+IrZx6bPxBFRUUcOXIk5rKmpiZGjx5Nfn4+W7duZeXKlQPezkAoEYiIxLBwbuWgKv5opaWlXHLJJbznPe8hLy+PcePGHVt21VVX8cADD9AlXQgAAAmkSURBVHDOOecwc+ZMLrrooiHbbjI0+qiIpIW3336bM888M9VhDItYvzXR6KO6RiAikuaUCERE0pwSgYhImlMiEBFJc0oEIiJpTolARCTN6T4CEZFoy2ZAa/2J8wvGwuLtA/rKxsZGfvGLX/D5z3++3+t+//vfZ9GiReTn5w9o233RGYGISLRYSSDR/CQM9HkE4CWCo0ePDnjbfdEZgYikn+eWQN2mga378Edjzx9/NixYGne1yGGoP/KRjzB27Fgee+wxOjo6uP766/na175Ga2srH//4x6mpqSEYDPLVr36V/fv3s3fvXj7wgQ9QVlbGq6++OrC4E1AiEBEZBkuXLuWtt95i/fr1vPjiizz++OOsWrUK5xzXXnstv//972loaGDChAk8++yzgDcGUXFxMd/73vd49dVXKSsr8yU2JQIRST8JjtwBuKc4/rKbnx305l988UVefPFF5s6dC0BLSwvbt2/nsssu4/bbb+eOO+7gmmuu4bLLLhv0tpKhRCAiMsycc9x555187nOfO2HZ2rVrWbFiBXfeeSdXXHEFd911l+/x6GKxiEi0grH9m5+EyGGor7zySh566CFaWloAqK2tpb6+nr1795Kfn8+NN97I7bffzrp1605Y1w86IxARiTbALqKJRA5DvWDBAj75yU9y8cXe084KCwv52c9+RlVVFYsXLyYjI4OsrCzuv/9+ABYtWsSCBQuoqKjw5WKxhqEWkbSgYag1DLWIiMShRCAikuaUCEQkbZxsTeEDMZDfqEQgImkhNzeXgwcPntLJwDnHwYMHyc3N7dd66jUkImlh4sSJ1NTU0NDQkOpQfJWbm8vEiRP7tY4SgYikhaysLKZMmZLqMEYkX5uGzOwqM9tmZlVmtiTGcjOze8PLN5rZeX7GIyIiJ/ItEZhZALgPWADMBm4ws9lRxRYAM8KvRcD9fsUjIiKx+XlGMB+ocs7tdM51Ao8C10WVuQ74ifOsBErMrMLHmEREJIqf1wgqgeqI6RrgwiTKVAL7IguZ2SK8MwaAFjPbNsCYyoADA1x3OIz0+GDkx6j4BkfxDc5Iju/0eAv8TAQWY150v61kyuCcexB4cNABma2Jd4v1SDDS44ORH6PiGxzFNzgjPb54/GwaqgEmRUxPBPYOoIyIiPjIz0SwGphhZlPMLBv4BPBMVJlngE+Few9dBDQ55/ZFf5GIiPjHt6Yh51y3md0GvAAEgIecc5vN7Jbw8geAFcDVQBVwFLjZr3jCBt285LORHh+M/BgV3+AovsEZ6fHFdNINQy0iIkNLYw2JiKQ5JQIRkTR3SiaCkTy0hZlNMrNXzextM9tsZl+MUeb9ZtZkZuvDL/+fXt17+7vNbFN42yc8Di7F+29mxH5Zb2bNZvalqDLDvv/M7CEzqzeztyLmjTGz35rZ9vD76DjrJvx79TG+ZWa2Nfxv+JSZlcRZN+Hfg4/x3WNmtRH/jlfHWTdV++9XEbHtNrP1cdb1ff8NmnPulHrhXZjeAUwFsoENwOyoMlcDz+Hdx3AR8MYwxlcBnBf+XAS8EyO+9wO/SeE+3A2UJViesv0X49+6Djg91fsPuBw4D3grYt63gSXhz0uAb8X5DQn/Xn2M7wogM/z5W7HiS+bvwcf47gFuT+JvICX7L2r5d4G7UrX/Bvs6Fc8IRvTQFs65fc65deHPR4C38e6mPpmMlKFBPgTscM7tScG2e3HO/R44FDX7OuDH4c8/BhbGWDWZv1df4nPOveic6w5PrsS7jycl4uy/ZKRs//UwMwM+DvxyqLc7XE7FRBBv2Ir+lvGdmU0G5gJvxFh8sZltMLPnzOysYQ3Mu7v7RTNbGx7eI9qI2H9496bE+8+Xyv3XY5wL3xcTfh8bo8xI2ZefwTvLi6Wvvwc/3RZuunooTtPaSNh/lwH7nXPb4yxP5f5LyqmYCIZsaAs/mVkh8ATwJedcc9TidXjNHecC/wUsH87YgEucc+fhjQ57q5ldHrV8JOy/bOBa4NcxFqd6//XHSNiXXwG6gZ/HKdLX34Nf7gemAXPwxh/7bowyKd9/wA0kPhtI1f5L2qmYCEb80BZmloWXBH7unHsyerlzrtk51xL+vALIMrOy4YrPObc3/F4PPIV3+h1pJAwNsgBY55zbH70g1fsvwv6eJrPwe32MMqn+W/w0cA3wty7coB0tib8HXzjn9jvngs65EPA/cbab6v2XCfwF8Kt4ZVK1//rjVEwEI3poi3B74v8CbzvnvhenzPhwOcxsPt6/08Fhiq/AzIp6PuNdUHwrqthIGBok7lFYKvdflGeAT4c/fxp4OkaZZP5efWFmVwF3ANc6547GKZPM34Nf8UVed7o+znZTtv/CPgxsdc7VxFqYyv3XL6m+Wu3HC69Xyzt4vQm+Ep53C3BL+LPhPTRnB7AJmDeMsV2Kd+q6EVgffl0dFd9twGa8HhArgfcOY3xTw9vdEI5hRO2/8Pbz8Sr24oh5Kd1/eElpH9CFd5T690Ap8DKwPfw+Jlx2ArAi0d/rMMVXhde+3vN3+EB0fPH+HoYpvp+G/7424lXuFSNp/4XnP9LzdxdRdtj332BfGmJCRCTNnYpNQyIi0g9KBCIiaU6JQEQkzSkRiIikOSUCEZE0p0Qg4rPwaKi/SXUcIvEoEYiIpDklApEwM7vRzFaFx43/bzMLmFmLmX3XzNaZ2ctmVh4uO8fMVkaM5T86PH+6mb0UHvBunZlNC399oZk9Hh7//+cRdz4vNbMt4e/5Top+uqQ5JQIRwMzOBP4Gb4CwOUAQ+FugAG9Mo/OA14C7w6v8BLjDOXcO3t2vPfN/DtznvAHv3ot3Nyp4o8x+CZiNd7fpJWY2Bm/ohLPC3/MNf3+lSGxKBCKeDwHnA6vDT5r6EF6FHeL4gGI/Ay41s2KgxDn3Wnj+j4HLw2PKVDrnngJwzrW742P4rHLO1ThvALX1wGSgGWgHfmRmfwHEHO9HxG9KBCIeA37snJsTfs10zt0To1yiMVliDYncoyPicxDvyWDdeCNRPoH30Jrn+xmzyJBQIhDxvAz8lZmNhWPPGz4d7//IX4XLfBL4o3OuCThsZpeF598EvOa850rUmNnC8HfkmFl+vA2Gn0lR7Lyhsr+EN+6+yLDLTHUAIiOBc26Lmf0b3pOkMvBGmbwVaAXOMrO1QBPedQTwhpV+IFzR7wRuDs+/CfhvM/v38Hf8dYLNFgFPm1ku3tnEPw/xzxJJikYfFUnAzFqcc4WpjkPET2oaEhFJczojEBFJczojEBFJc0oEIiJpTolARCTNKRGIiKQ5JQIRkTT3/wFHV8bcNvSr5gAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 30 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_filter(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "show_filter(network.params['W1'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-d74041f2720e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mshow_filter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"W2\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-7-b68401dd1e2e>\u001B[0m in \u001B[0;36mshow_filter\u001B[1;34m(filters, nx, margin, scale)\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m \u001B[0mhttps\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m//\u001B[0m\u001B[0mgist\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgithub\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcom\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0maidiary\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;36m7\u001B[0m\u001B[0md530d5e08011832b12\u001B[0m\u001B[1;31m#file-draw_weight-py\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \"\"\"\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[0mFN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mC\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mFH\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mFW\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfilters\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m     \u001B[0mny\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mceil\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mFN\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mnx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "network.params[\"W2\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}