{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "from function.layer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "        weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.216696723630171e-13\n",
      "b1:6.832173585560874e-13\n",
      "W2:8.136243804748476e-13\n",
      "b2:1.205702135353981e-10\n"
     ]
    }
   ],
   "source": [
    "# 정확도 검증\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.11003333333333333, 0.1157\n",
      "train acc, test acc | 0.9059833333333334, 0.9095\n",
      "train acc, test acc | 0.9263333333333333, 0.9297\n",
      "train acc, test acc | 0.9398, 0.9388\n",
      "train acc, test acc | 0.9456, 0.9445\n",
      "train acc, test acc | 0.95305, 0.9506\n",
      "train acc, test acc | 0.95855, 0.9559\n",
      "train acc, test acc | 0.9613333333333334, 0.9571\n",
      "train acc, test acc | 0.9655166666666667, 0.9599\n",
      "train acc, test acc | 0.9678, 0.962\n",
      "train acc, test acc | 0.97025, 0.9621\n",
      "train acc, test acc | 0.9729166666666667, 0.9639\n",
      "train acc, test acc | 0.9748, 0.9655\n",
      "train acc, test acc | 0.9739166666666667, 0.9661\n",
      "train acc, test acc | 0.9766166666666667, 0.9676\n",
      "train acc, test acc | 0.9784333333333334, 0.9672\n",
      "train acc, test acc | 0.9793333333333333, 0.9689\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼 파라메터\n",
    "iters_num = 10000  # 반복횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # print(i)\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 오차역전파법으로 기울기 계산\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1에폭 당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[[ -691.10526003  -152.64418021 -1010.50418955    10.98069209\n",
      "    405.45355296   139.73572846  -869.94122695   334.52955596\n",
      "    637.9572439   1178.55329027]]\n"
     ]
    }
   ],
   "source": [
    "a = network.predict(np.array(Image.open(\"./dataset/test.png\",).convert('L'), 'uint8').flatten().reshape((1, 784)))\n",
    "print(np.argmax(a))\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_norr = lambda T: (((T-T.min()) / (T.max() - T.min()))*255).astype('uint8').reshape(28, 28)\n",
    "plt.imshow(_norr(network.params[\"W1\"][:,19]), cmap='gray')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}